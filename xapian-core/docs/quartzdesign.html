<HTML>
<HEAD>
<TITLE>The Quartz Database Backend</TITLE>
</HEAD>
<BODY BGCOLOR="white">

<h1><center>The Quartz database backend</center></h1>

<h2>Status</h2>

WARNING: This document describes a piece of code which is still in
development.  As such, some of the descriptions within it will be
inconsistent with the code to be found in our releases and CVS
repository.  In such cases, the code is out of date if written before
this document (ie, before 20th October), and this document is out of
date if the code was written after this document.  If you find such
inconsistencies, especially this document being out of date, please
alert us to the problem using the discussion mailing list, at
&lt;xapian-discuss@lists.sourceforge.net&gt;.
<p>
Once the Quartz backend is stable, this comment should go away.

<h2>Why Quartz?</h2>

<em>
What is this thing called Quartz?  How does it fit in with the Xapian
library?</em>
<p>
Xapian can access information stored in various different formats,
some of which are legacy formats which can be read by Xapian but not
written to (for example, the Muscat 3.6 formats &quot;DA&quot; and
&quot;DB&quot;), some of which are essentially testing formats (such as the
InMemory format), and some which are experimental formats (such as the
Sleepycat format, which is fully featured but doesn't scale).
Each of these formats is comprised by a set of classes providing an interface
to a Database object and several other related objects (PostList, TermList,
etc...).

<p>
Quartz is simply the name of a backend which is currently being developed,
which draws from all our past experience to satisfy the following criteria:
<ul>
<li>
  Fast and scalable searches.
</li>
<li>
  May be updated (ie, database doesn't have to be built from scratch in order
  to make a single change).
</li>
<li>
  May be modified whilst searches are in progress.
</li>
<li>
  Provides atomic updates in the face of interruption at any point.
</li>
<li>
  Provides a single-writer, multiple-reader environment.
</li>
</ul>

<p>
Different backends can be optionally compiled into the Xapian library
(by specifying appropriate options to the configure script).  At the moment,
Quartz is not compiled by default since it is incomplete, but eventually it
will become a standard backend, included in all builds (unless explicitly
disabled).  In fact, once complete it may well become the only backend which
will be built by default.

<p>
<em>
Why do we call it Quartz - where does the name come from?
</em>
<p>Well, we had to call it something, and Quartz was
simply the first name we came up with which we thought we could live with...

<h2>Tables</h2>

A Quartz database consists of several tables, each of which stores a
different type of information: for example, one table stores the user-defined
data associated with each document, and another table stores the posting
lists (the lists of documents which particular terms occur in).
<p>
These tables consist of a set of key-tag pairs, which I shall often
refer to these as <em>items</em> or <em>entries</em>.  Items may
be accessed randomly by specifying a key and reading the item
pointed to, or in sorted order by creating a cursor pointing to a
particular item.  The sort order is a lexicographical ordering based
on the contents of the keys.  Only one instance of a key may exist in
a single table - inserting a second item with the same key as an existing
item will overwrite the existing item.
<p>
Positioning of cursors may be performed even when a
full key isn't known, by attempting to access an item which doesn't
exist: the cursor will then be set to point to the first item with a
key before that requested.
<p>
The QuartzTable class defines the standard interface to tables.  This
has two subclasses - the QuartzDiskTable and QuartzBufferedTable 
interface.  The former provides direct access to the table as stored
on disk, and the latter provides access via a large buffer in order to
use the memory as a write cache and greatly speed the process of
indexing.

<h2>The contents of the tables</h2>

We shall worry about the implementation of the tables later, but first
we shall look at what is stored within each table.
<p>
There are six tables comprising a quartz database.
<ul>
<li>
<B>Record</B>.
This stores the arbitrary chunk of data associated with each document.
<p>
It also stores a couple of special fields - one containing the next
document ID to use when adding a document (document IDs are allocated in
increasing order, starting at 1, and are currently never reused), and one
field containing the total length of the documents in the database.  This
latter quantity is used for calculation of the average document length, and
hence for calculation of normalised document lengths.
<p>
</li><li>
<B>Attribute</B>.
This stores a set of attributes for each document 
<p>
Currently, there is one item for each document in the database, which
consists of a list of keyno's and attribute values for that document.  The
alternative implementation is to store an item for each attribute, whose
key is a combination of the document ID and the keyno, and whose tag is the
attribute value.
<p>
Which implementation is better depends on the access pattern: if a document
is being passed across a network link, all the attributes for a document
are read - if a document is being dealt with locally, usually only some of
the attributes will be read.
<p>
Documents will usually have very few attribute values, so the current
implementation may actually be the most suitable.
<p>
</li><li>
<b>Lexicon</b>.
This stores some information about each term in the database in a fast
access form.
<p>
For each term in the database, this has an entry whose key is the term, and
whose contents are the term ID for that term, and the term frequency for
that term.  This is intended to be a fast access table, so that processes
can determine the term ID or weight of a term quickly, whether for the
purpose of doing an expand or for the purpose of selecting important terms.
<p>
The term IDs stored in the lexicon are used (only) in the keys used to
access items in the PostList and PositionList table.  The termIDs allow the
size of the keys for these tables to be bounded, and may also allow better
compression.  It is debatable whether the termIDs are a good idea, however,
or whether it would be better simply to use the termnames instead.
<p>
This table also has a special entry, which stores the next term ID to
allocate.  Like document IDs, term IDs are allocated in increasing order,
starting at 1, and are currently never reused.
<p>
Speed of access to this table is likely to be critical - attempting to
ensure that the most frequently used part of the lexicon is always cached
in memory to a large extent is likely to give high rewards.
<p>
</li><li>
<b>TermList</b>.
This stores the list of terms which appear in a document.
<p>
The list first stores the document length, and the number of entries in the
termlist (this latter value is stored for quick access - it could also be
determined by running through the termlist).  It then stores a set of
entries: each entry in the list consists of a term (as a string), and the
wdf (within document frequency - how many times the term appears in the
document) of that term.
<p>
In a non-modifiable database, the term frequency could be stored in the
termlist for each entry in each list.  This would enable query expansion
operations to occur significantly faster by avoiding the need for a large
number of extra lookups - however, this cannot be implemented in a
writable database without causing any modifications to modify a very large
proportion of the database.
<p>
</li><li>
<b>PositionList</b>.
For each term / document pair, this stores the list of positions in the
document at which the term occurs.
<p>
</li><li>
<b>PostList</b>.
This stores the list of documents in which each term appears.
<p>
</li>
</ul>

(Note that the PositionList and PostList are not yet implemented.)

<h2>Representation of integers, strings, etc.</h2>

It is well known that in modern computers there are many, many CPU cycles
for each disk read, or even memory read.  It is therefore important to
minimse disk reads, and can be advantageous to do so even at the expense of
a large amount of computation.  In other words, <em>Compression is
good</em>. ;-)
<p>
(The current implementation doesn't make use of any real compression.
However, it has been designed to scan through lists of data in such a way
as to facilitate introducing proper compression.)
<p>
It is planned to use various compression techniques, for example:
<ul>
<li>
In posting lists, successive document IDs will be compressed by a difference
encoding, and then by some further scheme, such as a huffman encoding, or
similar.
<p>
</li><li>
In position lists, successive positions will be encoded similarly.
<p>
</li><li>
In termlists, terms are stored as string values in sorted order - a
string difference can be used to reduce storage size.
<p>
</li>
</ul>
<p>
The only compression currently performed is, whereever an unsigned integer
is stored in a table, to represent it in a simply encoded form as:
<ol>
<li>
First byte: if integer is &lt; 128, store integer, otherwise store
integer modulo 128, but with top bit set.
</li><li>
Shift integer right 7 places.
</li><li>
Second byte: if integer is &lt; 128, store integer, otherwise store
integer modulo 128, but with top bit set.
</li><li>
Shift integer right 7 places.
</li><li>
etc...
</li>
</ol>

<h2>PostLists and chunks.</h2>

Posting lists can grow to be very large - some terms occur in a very large
proportion of the documents, and their posting lists can represent a
significant fraction of the size of the whole database.  Therefore, we do
not wish to read an entire posting list into memory at once.  (Indeed, we'd
rather only read a small portion of it at all, but that's a different
story - see the documentation on optimisation of the matcher [which doesn't
exist yet, ed.]).
<p>
To deal with this, we store posting lists in small chunks, each the right
size to be stored in a single B-tree block, and hence to be accessed with a
minimal amount of disk latency.  
<p>
The key for the first chunk in a posting list is the term ID of the term
whose posting list it is.  The key in subsequent chunks is the term ID
followed by the document ID of the first document in the chunk.  This
allows the cursor methods to be used to scan through the chunks in order,
and also to jump to the chunk containing a particular document ID.

<p>
It is quite possible that data in other tables (eg, termlist and possibly
position lists) would benefit from being split into chunks in this way.

<h2>Btree implementation</h2>

The tables are currently all planned to be implemented as B-trees.
<p>
(The code currently uses a temporary hack to implement tables by reading the
contents of an unstructured file into memory, modifying it, and writing it all
back again.  Martin is working on a B-tree manager, which is complete apart
from some API issues and will be added into the CVS repository within the next
couple of weeks.)
<p>
In some situations, the use of a different structure would be appropriate - in
particular for the lexicon where key ordering is irrelevant, and a hashing
scheme would likely provide more memory and time efficient access.  This
will be investigated once the initial version is fully functional.
<p>
A B-tree is a fairly standard structure for storing this kind of data, so I
will not describe it in detail - see a reference book on database design and
algorithms for that.  The essential points are that it is a block-based
multiply branching tree structure, storing keys in the internal blocks and
key-tag pairs in the leaf blocks.
<p>
Our implementation is fairly standard, except for its revision scheme,
which allows modifications to be applied atomically whilst other processes
are reading the database.  This scheme involves copying each block in the
tree which is involved in a modification, rather than modifying it in
place, so that a complete new tree structure is built up whilst the old
structure is unmodified (although this new structure will typically share a
large number of blocks with the old structure).  The modifications can then
be atomically applied by writing the new root block and making it active.
<p>
After a modification is applied successfully, the old version of the
table is still fully intact, and can be accessed.  The old version only
becomes invalid when a second modification is attempted (and it becomes
invalid whether or not that second modification succeeds).
<p>
There is no need for a process which is writing the database to know
whether any processes are reading previous versions of the database.  As long
as only one update is performed before the reader closes (or reopens) the
database, no problem will occur.  If more than one update occurs whilst
the table is still open, the reader will notice that the database has been
changed whilst it has been reading it by comparing a revision number stored
at the start of each block with the revision number it was expecting.  An
appropriate action can then be taken (for example, to reopen the database
and repeat the operation).
<p>
An alternative approach would be to obtain a read-lock on the revision
being accessed.  A write would then have to wait until no read-locks
existed on the old revision before modifying the database.

<h2>Buffered tables</h2>

If each change to a table (ie, modification of a key-tag pair) was
immediately written to disk, there would be two problems.

<ol>
<li>
The system would be very slow.  If a disk read and then a write was
required each time an item was changed, the indexing process would
spend most of its time waiting for the disk's write head to seek to
the appropriate block.  By buffering up a large set of changes, and
then writing them all out in a sorted order, seeking is minimised.
</li>
<li>
Operations which involve modifying more than one key-tag pair would not
be atomic.  For example, when adding a document to a database the record
table has three items updated - one containing the data stored in the
document, one which stores the next document ID to allocate, and one which
stores the sum of all the document lengths in the database.  These three
items must be updated at the same time, so that a consistent state is always
seen by other processes, and a consistent state remains if the system
is terminated unexpectedly.
</li>
</ol>

<p>
As a result, the QuartzBufferedTable object is available.  This simply
stores a set of modified entries in memory, applying them to disk only
when the apply method is called.
<p>
In fact, a QuartzBufferedTable object has to have two handles open on
the table - one for reading and one for writing.  This is simply
because the interface for writing a table is more limited than that
for reading a table (in particular, cursor operations are not
available).
<p>
(Note: the current temporary implementation of quartz tables doesn't
use two handles.  It doesn't implement Cursor operations yet either.)

<h2>Applying changes to all the tables simultaneously</h2>

This is all wonderful: we have tables storing arbitrary bits of useful
data, we can update bits of them as we like, and we can then call a
method and have all the modifications applied to the table atomically.
Unfortunately, we need more than that - we need to be able to apply
modifications as a single atomic transaction across multiple tables, so
that the tables are always accessed in a mutually consistent state.
<p>
The revisioning scheme described earlier comes to the rescue!  By carefully
making sure that we open all the tables at the same revision, and by ensuring
that at least one such consistent revision always exists, we can extend the
scope of atomicity to cover all the tables.  In detail:

<ul>
<li>
When opening a database, we open each table in a specified order: lets call
the tables <em>A</em>, <em>B</em>, <em>C</em>, <em>D</em>, <em>E</em>,
<em>F</em>, and say we open them in alphabetical order.
</li><li>
When opening a database, after opening the first table, <em>A</em>, at the
newest available revision, we read its revision number.  We then open all
the other tables at the same revision number.
</li><li>
When writing changes to a database, we write the tables in reverse order.
</li>
</ul>

This scheme guarantees that modifications are atomic across all the tables
- essentially we have made the modification get committed only when the
final table is committed.

<h2>Items to be added to this document</h2>

<ul>
<li>
Describe that postlists must be stored in sorted order, for boolean queries, so cannot store in reverse wdf order for efficiency.  A possible workaround is to
store the postlists in two or more chunks, ordered by wdf, and to access them
in this order.
</li>
<li>
An better explanation of why there will always be a consistent set of table
versions using the scheme described above.
</li>
<li>
Mention that future versions will allow the database creator to decide
whether to store certain levels of detail in the database - eg, whether to
store document lengths, term frequencies in the termlists, document lengths
in the posting lists, etc.
</li>
<li>
Mention that when writing to a database, items are kept in the packed form
they are stored in on disk and simply scanned through, modified and written
on-the-fly.  This minimises memory use, allowing more data to be cached,
and hopefully keeping performance high.
</li>
<li>
Add comment about the inversion process - we are essentially doing a text
based partitioning scheme.
</li>
<li>
Describe thread locking system.
</li>
<li>
Mention adding documents: usual to add with a new document ID which is
greater than any currently in the system.  This means that new postings
get added to the end of posting lists (so we make it easy to get to the
end of a posting list quickly).  Also, we key position lists by document
ID first and termname second, so that new positionlists get added to the
end of the positionlist database, meaning that hardly any blocks will need
to be altered in this database: data just gets added to the end.
</li>
</ul>

<h2>Endnote</h2>

The system as described could, no doubt, be improved in several ways.
If you can think of such ways then suggest it to us,
so we can have a discussion of the improvment to see whether it would
help: if it would we will add it to the design (and eventually the
code) - if not, we'll add a discussion about it to this document.

<!-- FOOTER $Author$ $Date$ $Id$ -->
</BODY>
</HTML>
